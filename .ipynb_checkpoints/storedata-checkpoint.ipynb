{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import psycopg2, time\n",
    "from cleanup import clean\n",
    "\n",
    "# Variables:\n",
    "# Dicts\n",
    "tags = {}\n",
    "authordict = {}\n",
    "\n",
    "# Sets\n",
    "new_keywords = set()\n",
    "new_domains = set()\n",
    "new_authors = set()\n",
    "\n",
    "# Lists \n",
    "new_types = []\n",
    "\n",
    "# Make connection to database\n",
    "connection = psycopg2.connect(\n",
    "    user = \"athanar\",\n",
    "    host = \"localhost\",\n",
    "    port = \"5432\",\n",
    "    database = \"datascience\")\n",
    "connection.set_client_encoding('UTF8')\n",
    "cursor = connection.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "start_time = time.time()\n",
    "reader = pd.read_csv(\n",
    "    \"1mio-raw.csv\", \n",
    "    encoding='utf-8', \n",
    "    chunksize=10000)\n",
    "\n",
    "# Read SQL file\n",
    "def executeScriptFromFile(filename):\n",
    "    fd = open(filename, 'r')\n",
    "    sqlFile = fd.read()\n",
    "    fd.close()\n",
    "    sqlCommands = sqlFile.split(';')\n",
    "    for command in sqlCommands:\n",
    "        try:\n",
    "            cursor.execute(command)\n",
    "        except:\n",
    "            continue  \n",
    "executeScriptFromFile('create_tables.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inserts dataframe into database\n",
    "def insertTable(cols, vals, target):\n",
    "    sql = \"INSERT INTO \"+target+\" (\" +cols + \") VALUES (\" + \"%s,\"*(len(vals.iloc[0])-1) + \"%s)\"\n",
    "    cursor.executemany(sql, vals.values.tolist())\n",
    "    connection.commit()\n",
    "\n",
    "# Get typeid for type string\n",
    "def typeLookup(typeval):\n",
    "    if (isinstance(typeval, float)):\n",
    "        return 12\n",
    "    else:\n",
    "        return new_types.index(typeval) + 1\n",
    "\n",
    "# Extract comma separated parts of string column\n",
    "def extractParts(dict, ids, column):\n",
    "    tmp = []\n",
    "    for i in range(len(column)):\n",
    "        if (isinstance(column.iloc[i], float)):\n",
    "            tmp.extend(str(column.iloc[i]))\n",
    "            dict[ids.iloc[i]] =  str(column.iloc[i])\n",
    "        elif (column.iloc[i] == \"[\\'\\']\"):\n",
    "            continue\n",
    "        else:\n",
    "            new_vals = (column.iloc[i]\n",
    "                        .replace('[', '')\n",
    "                        .replace(']', '')\n",
    "                        .replace('\\'', '')\n",
    "                        .replace('\\\"', '')\n",
    "                        .lower()\n",
    "                        .split(', '))\n",
    "            tmp.extend(new_vals)\n",
    "            dict[ids.iloc[i]] = new_vals\n",
    "    return set(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "i = 1\n",
    "for data in reader:\n",
    "    # Size; Highly temporary for testing purposes\n",
    "    if (i >= 3):\n",
    "        break\n",
    "\n",
    "    # Clean data\n",
    "    #data['content'] = data['content'].apply(clean)\n",
    "    data['title'] = data['title'].apply(clean)\n",
    "    data['summary'] = data['summary'].apply(clean)\n",
    "\n",
    "    # Fetches article from dataframe\n",
    "    article = data.iloc[:,[1,9,5,15,6,7,8]]\n",
    "    articleval = \"articleID, title, content, summary, scrapedAt, insertedAt, updatedAt\"\n",
    "    insertTable(articleval, article, \"Article\")\n",
    "\n",
    "    # Fetches types from dataframe, done like this as the first round \n",
    "    # finds all relevant types \n",
    "    if (len(new_types) < 1):\n",
    "        types = data['type'].drop_duplicates().dropna()\n",
    "        typeframe = pd.DataFrame(types).rename(columns={'type':'typeValue'})\n",
    "        new_types = list(types)\n",
    "        insertTable(\"typeValue\", pd.DataFrame(types), \"Types\") \n",
    "\n",
    "    # Fills Typelinks\n",
    "    articleid = data.iloc[:,[1]]\n",
    "    typeid = data['type'].apply(typeLookup)\n",
    "    typelinks = pd.concat([articleid, typeid], axis=1, ignore_index=True)\n",
    "    insertTable(\"articleID, typeID\", typelinks, \"Typelinks\")\n",
    "\n",
    "    # Fetches keywords from dataframe and inserts new keywords\n",
    "    keywords = extractParts(tags, data['id'],data['meta_keywords'])\n",
    "    keyword_list = list(keywords.difference(new_keywords))\n",
    "    new_keywords = new_keywords.union(keywords)  \n",
    "    insertTable(\"keywordValue\", pd.DataFrame(keyword_list), \"Keyword\")\n",
    "\n",
    "    # Fetches domain from dataframe and inserts new domains\n",
    "    domain = set(data.loc[:,'domain'])\n",
    "    domain_list = list(domain.difference(new_domains))\n",
    "    new_domains = new_domains.union(domain)\n",
    "    insertTable(\"domainURL\", pd.DataFrame(domain_list), \"Domain\")\n",
    "\n",
    "    # Fetches webpageURL from dataframe and inserts\n",
    "    dom_list = list(new_domains)\n",
    "    new_webs = data['domain'].apply(lambda x: dom_list.index(x)+1)\n",
    "    dom_frame = pd.DataFrame(\n",
    "        {'id': data['id'], 'domain': new_webs, 'url': data['url']})\n",
    "    insertTable(\"articleID, domainID, webpageurl\", dom_frame, \"Webpage\")\n",
    "\n",
    "    # Fetches authors from dataframe and inserts new authors\n",
    "    authors = extractParts(authordict, data['id'], data['authors'])\n",
    "    author_list = list(authors.difference(new_authors))\n",
    "    new_authors = new_authors.union(authors)\n",
    "    insertTable(\"authorName\", pd.DataFrame(author_list), \"Author\")\n",
    "\n",
    "    # Round counter for timing\n",
    "    print(\"Round %d took %s seconds\" % (i,time.time() - start_time))\n",
    "    i = i+1\n",
    "\n",
    "# Inserts tags into tag table\n",
    "keyword_listform = list(new_keywords)\n",
    "tmp_lst_kw = []\n",
    "for k, v in tags.items():\n",
    "    for kword in v:\n",
    "        tmp_lst_kw.append([k, keyword_listform.index(kword)+1])\n",
    "insertTable(\"articleID, keywordID\", pd.DataFrame(tmp_lst_kw), \"Tags\")\n",
    "print(\"Keywords took %s seconds\" % (time.time() - start_time))\n",
    "\n",
    "# Inserts into WrittenBy table\n",
    "author_listform = list(new_authors)\n",
    "tmp_lst_aut = []\n",
    "for k, v in authordict.items():\n",
    "    for kword in v:\n",
    "        tmp_lst_aut.append([k, author_listform.index(kword)+1])\n",
    "insertTable(\"articleID, authorID\", pd.DataFrame(tmp_lst_aut), \"WrittenBy\")\n",
    "\n",
    "print(\"Finished. Took %s seconds\" % (time.time() - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query():\n",
    "    # Read SQL file\n",
    "    def executeScriptFromFile(filename):\n",
    "        fd = open(filename, 'r')\n",
    "        sqlFile = fd.read()\n",
    "        fd.close()\n",
    "        sqlCommands = sqlFile.split(';')\n",
    "        for command in sqlCommands:\n",
    "            try:\n",
    "                cursor.execute(command)\n",
    "                print(pd.DataFrame(cursor.fetchall()))\n",
    "            except:\n",
    "                continue \n",
    "\n",
    "    executeScriptFromFile('queries.sql')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'new_types' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-e5b39db4dfcd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mstoredata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-4-415af7cf9b71>\u001b[0m in \u001b[0;36mstoredata\u001b[1;34m()\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Fetches types from dataframe, done like this as the first round\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# finds all relevant types\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m             \u001b[0mtypes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m             \u001b[0mtypeframe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtypes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrename\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[1;34m'type'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'typeValue'\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUnboundLocalError\u001b[0m: local variable 'new_types' referenced before assignment"
     ]
    }
   ],
   "source": [
    "storedata()\n",
    "query()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
